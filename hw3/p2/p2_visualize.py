# -*- coding: utf-8 -*-
"""dlcv_hw3_p2_visualize.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xPQNjDQbs3vEN7RltPAEzRGpzRUVBRwL
"""

# !gdown 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR -O hw3_data.zip
# !unzip hw3_data.zip

# !unzip temp.zip

# !pip install timm

import os
import json
import torch
import torch.nn as nn
from torch.nn import MultiheadAttention
import timm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision.transforms import functional as TF
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm
import torchvision.transforms as transforms

from tokenizer import BPETokenizer

tokenizer = BPETokenizer("./encoder.json", "vocab.bpe")

class InferenceDataset:
    def __init__(self, data_dir):
        super(InferenceDataset, self).__init__()
        self.data_dir = data_dir
        self.files = sorted([p for p in os.listdir(data_dir)])
        self.transform = transforms.Compose(
            [
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )

    def __len__(self):
        return len(self.files)
    def __getitem__(self, idx):
        fname = self.files[idx]
        img = Image.open(os.path.join(self.data_dir, fname)).convert('RGB')  # Convert to RGB
        img = self.transform(img)
        return img, fname

import math
import collections
import torch
from torch import nn, Tensor
import torch.nn.functional as F

class Config:
    def __init__(self, checkpoint=None):
        self.n_layer = 12
        self.n_head = 12
        self.n_embd = 768
        self.vocab_size = 50257
        self.block_size = 1024
        self.checkpoint = checkpoint

class CrossAttention(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.embed_dim = cfg.n_embd
        self.num_heads = cfg.n_head
        self.head_dim = self.embed_dim // self.num_heads
        assert self.embed_dim % self.num_heads == 0, "Embedding dimension must be divisible by number of heads"
        # self.q = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.k = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.v = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.c_proj = lora.Linear(self.embed_dim, self.embed_dim, r=16)

        self.q = nn.Linear(self.embed_dim, self.embed_dim)
        self.k = nn.Linear(self.embed_dim, self.embed_dim)
        self.v = nn.Linear(self.embed_dim, self.embed_dim)
        self.scale = self.head_dim ** -0.5
        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)

    def forward(self, x, context):
        b, t, _ = x.size()
        q = self.q(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k(context).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v(context).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)
        attn_scores = (q @ k.transpose(-2, -1)) * self.scale
        # print(f"attn_scores.shape: {attn_scores.shape}")
        # print((self.bias[:, :, :t, :t] == 0).shape)
        # attn_scores = attn_scores.masked_fill(self.bias[:, :, :t, :t] == 0, float('-inf'))

        attn_probs = F.softmax(attn_scores, dim=-1)
        # print(f"attn_probs.shape: {attn_probs.shape}")

        return self.c_proj((attn_probs @ v).transpose(1, 2).contiguous().view(b, t, self.embed_dim)), attn_probs


class Attention(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        # self.c_attn = lora.Linear(cfg.n_embd, 3 * cfg.n_embd, r=32)
        # self.c_proj = lora.Linear(cfg.n_embd, cfg.n_embd, r=16)
        self.c_attn = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)
        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)
        self.n_head = cfg.n_head
        self.n_embd = cfg.n_embd
        size = cfg.block_size
        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))

    def forward(self, x):
        B, T, C = x.size() # batch, context, embedding
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))

class Block(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.ln_1 = nn.LayerNorm(cfg.n_embd)
        self.ln_2 = nn.LayerNorm(cfg.n_embd)
        self.ln_3 = nn.LayerNorm(cfg.n_embd)

        self.attn = Attention(cfg)
        self.cross_attn = CrossAttention(cfg)
        self.mlp = nn.Sequential(collections.OrderedDict([
            # ('c_fc', lora.Linear(cfg.n_embd, 4 * cfg.n_embd, r=32)),
            ('c_fc', nn.Linear(cfg.n_embd, 4 * cfg.n_embd)),
            ('act', nn.GELU(approximate='tanh')),
            # ('c_proj', lora.Linear(4 * cfg.n_embd, cfg.n_embd, r=32)),
            ('c_proj', nn.Linear(4 * cfg.n_embd, cfg.n_embd))
        ]))
        # only add lora in the last few mlp blocks

    def forward(self, x, context):
        x = x + self.attn(self.ln_1(x))
        # x = x + self.cross_attn(self.ln_1(x), context)
        # x = x + self.mlp(self.ln_2(x))
        x0, attn_prob = self.cross_attn(self.ln_2(x), context)
        x = x + x0
        x = x + self.mlp(self.ln_3(x))
        return x, attn_prob

class Decoder(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.block_size = cfg.block_size
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(cfg.vocab_size, cfg.n_embd),
            wpe = nn.Embedding(cfg.block_size, cfg.n_embd),
            h = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)]),
            ln_f = nn.LayerNorm(cfg.n_embd)
        ))
        # self.lm_head = lora.Linear(cfg.n_embd, cfg.vocab_size, r=16)
        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight
        # load checkpoint
        if self.cfg.checkpoint is not None:
            state_dict = torch.load(self.cfg.checkpoint)
            transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]
            for key, value in state_dict.items():
                if any(key.endswith(w) for w in transposed):
                    state_dict[key] = value.t()
            self.transformer.load_state_dict(state_dict, strict=False)

    def forward(self, x: Tensor, encoder_output: Tensor):
        x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))
        pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)
        x = self.transformer.wte(x) + self.transformer.wpe(pos)
        # x = self.lm_head(self.transformer.ln_f(self.transformer.h(x)))
        attn_probs = 0
        for block in self.transformer.h:
            x0, attn_probs = block(x, encoder_output)  # Pass both x and encoder_output to each block
            x = x0

        x = self.lm_head(self.transformer.ln_f(x))
        # x = self.lm_head(self.transformer.ln_f(self.transformer.h(x, encoder_output)))
        return x, attn_probs
class VisionLanguageModel(nn.Module):
    def __init__(self, decoder_cfg):
        super(VisionLanguageModel, self).__init__()
        self.vit = timm.create_model('vit_large_patch14_224_clip_laion2b', pretrained=True, num_classes=0)
        self.decoder = Decoder(decoder_cfg)
        self.output_reshape = nn.Linear(1024, 768)

    def forward(self, images, input_ids):
        x = self.vit.forward_features(images)
        x = self.output_reshape(x)
        decoder_output, attn_probs = self.decoder(input_ids, x)
        return decoder_output, attn_probs

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"using device {device}")

config = Config(checkpoint = "./hw3_data/p2_data/decoder_model.bin")
model = VisionLanguageModel(config).to(device)

model.load_state_dict(torch.load("./drive/MyDrive/DLCV/hw3/p2/p2_1_model.bin"), strict=False)

# test_image_folder = './hw3_data/p3_data/images'
# test_dataset = InferenceDataset(test_image_folder)
# print("test_dataset size:", len(test_dataset))
# test_loader = DataLoader(test_dataset, batch_size=16)

val_image_folder = '/content/hw3_data/p3_data/images'
val_dataset = InferenceDataset(val_image_folder)
print("val Dataset size:", len(val_dataset))
val_loader = DataLoader(val_dataset, 1)
# val_image_folder = './hw3_data/p2_data/images/val'
# val_dataset = InferenceDataset(val_image_folder)
# print("val Dataset size:", len(val_dataset))
# val_loader = DataLoader(val_dataset, 16)

def generate(model, image, sequence, max_tokens=70, temperature=1.0):
    for _ in range(max_tokens):
        out, attn_prob = model.forward(image, sequence)
        out = out[:, -1, :] / temperature
        probs = F.softmax(out, dim=-1)
        next_token = torch.argmax(probs, dim=-1, keepdim=True)
        sequence = torch.cat([sequence, next_token], dim=1)
        if next_token.item() == 50256:
            break
    attn_prob = attn_prob[:, -1, -1, 1:]  # Reshape to [1, 1, 1, 256]
    return sequence.cpu().flatten(), attn_prob

from google.colab import drive
drive.mount('/content/drive')

"""# visualize"""

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

def visualize_attention(ax, image, attn_map, token_idx, patch_size=16, num_patches=16):
    # Convert the PyTorch tensor image to a PIL Image for easier manipulation
    image = Image.fromarray(image.permute(1, 2, 0).cpu().detach().numpy().astype(np.uint8))

    # Normalize the attention map and resize it to the image size
    attn_map = attn_map.reshape(num_patches, num_patches)
    attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())
    attn_map_resized = Image.fromarray(np.uint8(255 * attn_map), 'L')
    attn_map_resized = attn_map_resized.resize(image.size, Image.BILINEAR)

    # Create a heatmap from the attention map
    ax.imshow(image)
    if token_idx != 0:
        ax.imshow(attn_map_resized, cmap='jet', alpha=0.5)  # Overlay the heatmap on the image
    ax.axis('off')
    ax.set_title(f"{word}")
model.eval()
initial_sequence = torch.tensor([50256]).unsqueeze(0).to(device)
predictions ={}
for images, filenames in tqdm(val_loader):
    images = images.to(device)

    for image, filename in zip(images, filenames):
        sequence, attn_prob = generate(model, image.unsqueeze(0), initial_sequence)
        tts = sequence.tolist()[1:]
        if 50256 in tts:
            tts = tts[:tts.index(50256)]
        decoded_sequence = tokenizer.decode(tts)  # Decode the sequence of tokens into text

        attn_prob_reshaped = attn_prob.reshape(1, 1, 16, 16).detach()
        attn_map_resized = attn_prob_reshaped.squeeze().cpu().numpy()

        # Determine the number of subplots needed
        num_tokens = sequence.shape[0]
        num_cols = 4  # You can adjust this number
        num_rows = (num_tokens + num_cols - 1) // num_cols

        # Create a figure with subplots
        fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 5))
        axes = axes.flatten()

        filename = filename.split('.')[0]
        predictions[filename] = decoded_sequence

        for token_idx in range(num_tokens):
            # Use the token at the current index to get the corresponding word
            token = sequence[token_idx].item()
            word = tokenizer.decode([token]) if token != 50256 else "<|endoftext|>"
            visualize_attention(axes[token_idx], image, attn_map_resized, word, num_patches=16)

        # Turn off any unused subplots
        for ax in axes[num_tokens:]:
            ax.axis('off')

        plt.tight_layout()
        plt.savefig(f"visualization_{filename}.png")
        plt.close(fig)

with open("p3img_prediction.json", 'w') as f:
    json.dump(predictions, f, indent=4)

"""# inference"""

model.eval()
initial_sequence = torch.tensor([50256]).unsqueeze(0).to(device)
predictions ={}
for images, filenames in tqdm(val_loader):
    images = images.to(device)

    for image, filename in zip(images, filenames):
        sequence, _ = generate(model, image.unsqueeze(0), initial_sequence)
        tts = sequence.tolist()[1:]
        if 50256 in tts:
            tts = tts[:tts.index(50256)]
        decoded_sequence = tokenizer.decode(tts)

        filename = filename.split('.')[0]
        predictions[filename] = decoded_sequence

with open("p2_1_bin_val_prediction.json", 'w') as f:
    json.dump(predictions, f, indent=4)



# !pip install git+https://github.com/openai/CLIP.git
import clip
# !pip install git+https://github.com/bckim92/language-evaluation.git
import language_evaluation
from collections import defaultdict

def readJSON(file_path):
    try:
        with open(file_path) as f:
            data = json.load(f)
        return data
    except:
        return None

def getGTCaptions(annotations):
    img_id_to_name = {}
    for img_info in annotations["images"]:
        img_name = img_info["file_name"].replace(".jpg", "")
        img_id_to_name[img_info["id"]] = img_name

    img_name_to_gts = defaultdict(list)
    for ann_info in annotations["annotations"]:
        img_id = ann_info["image_id"]
        img_name = img_id_to_name[img_id]
        img_name_to_gts[img_name].append(ann_info["caption"])
    return img_name_to_gts


class CIDERScore:
    def __init__(self):
        self.evaluator = language_evaluation.CocoEvaluator(coco_types=["CIDEr"])

    def __call__(self, predictions, gts):
        """
        Input:
            predictions: dict of str
            gts:         dict of list of str
        Return:
            cider_score: float
        """
        # Collect predicts and answers
        predicts = []
        answers = []
        for img_name in predictions.keys():
            predicts.append(predictions[img_name])
            answers.append(gts[img_name])

        # Compute CIDEr score
        results = self.evaluator.run_evaluation(predicts, answers)
        return results['CIDEr']


class CLIPScore:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()
        self.max_score = float('-inf')  # Initialize with negative infinity
        self.min_score = float('inf')   # Initialize with positive infinity
        self.max_img_name = None
        self.min_img_name = None

    def __call__(self, predictions, images_root):
        """
        Input:
            predictions: dict of str
            images_root: str
        Return:
            clip_score: float, max_img_name: str, min_img_name: str
        """
        total_score = 0.
        num_predictions = len(predictions)

        for img_name, pred_caption in predictions.items():
            image_path = os.path.join(images_root, f"{img_name}.jpg")
            image = Image.open(image_path).convert("RGB")

            clip_score = self.getCLIPScore(image, pred_caption)
            total_score += clip_score

            # Update max and min scores and their corresponding image names
            if clip_score > self.max_score:
                self.max_score = clip_score
                self.max_img_name = img_name

            if clip_score < self.min_score:
                self.min_score = clip_score
                self.min_img_name = img_name

        average_score = total_score / num_predictions
        return average_score, self.max_img_name, self.max_score, self.min_img_name, self.min_score

    def getCLIPScore(self, image, caption):
        """
        This function computes CLIPScore.
        Input:
            image: PIL.Image
            caption: str
        Return:
            clip_score: float
        """
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        text_input = clip.tokenize([caption]).to(self.device)

        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            text_features = self.model.encode_text(text_input)

        cos_sim = torch.nn.functional.cosine_similarity(image_features, text_features).item()
        return 2.5 * max(cos_sim, 0)

# class CLIPScore:
#     def __init__(self):
#         self.device = "cuda" if torch.cuda.is_available() else "cpu"
#         self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
#         self.model.eval()


#     def __call__(self, predictions, images_root):
#         """
#         Input:
#             predictions: dict of str
#             images_root: str
#         Return:
#             clip_score: float
#         """
#         total_score = 0.

#         for img_name, pred_caption in predictions.items():
#             image_path = os.path.join(images_root, f"{img_name}.jpg")
#             image = Image.open(image_path).convert("RGB")

#             total_score += self.getCLIPScore(image, pred_caption)
#         return total_score / len(predictions)

#     def getCLIPScore(self, image, caption):
#         """
#         This function computes CLIPScore based on the pseudocode in the slides.
#         Input:
#             image: PIL.Image
#             caption: str
#         Return:
#             cilp_score: float
#         """
#         image_input = self.preprocess(image).unsqueeze(0).to(self.device)
#         text_input = clip.tokenize([caption]).to(self.device)

#         with torch.no_grad():
#             image_features = self.model.encode_image(image_input)
#             text_features = self.model.encode_text(text_input)

#         cos_sim = torch.nn.functional.cosine_similarity(image_features, text_features).item()
#         return 2.5 * max(cos_sim, 0)

pred_file = "./p2_1_bin_val_prediction.json"
annotation_file = "/content/hw3_data/p2_data/val.json"
# Read data
predictions = readJSON(pred_file)
annotations = readJSON(annotation_file)
images_root = "hw3_data/p2_data/images/val"
# Preprocess annotation file
gts = getGTCaptions(annotations)

# Check predictions content is correct
assert type(predictions) is dict
assert set(predictions.keys()) == set(gts.keys())
assert all([type(pred) is str for pred in predictions.values()])

# CIDErScore
cider_score = CIDERScore()(predictions, gts)
print(cider_score)

# CLIPScore
clip_score, max_img, max_score, min_img, min_score = CLIPScore()(predictions, images_root)
print(f"max_img:{max_img}, max_score:{max_score}")
print(f"min_img:{min_img}, min_score:{min_score}")
print(f"CIDEr: {cider_score} | CLIPScore: {clip_score}")

pred_file = "/content/p3_umbrella.json"
annotation_file = "/content/hw3_data/p2_data/val.json"
# Read data
predictions = readJSON(pred_file)
# annotations = readJSON(annotation_file)
images_root = "hw3_data/p3_data/images"
# Preprocess annotation file
# gts = getGTCaptions(annotations)

# Check predictions content is correct
assert type(predictions) is dict
# assert set(predictions.keys()) == set(gts.keys())
assert all([type(pred) is str for pred in predictions.values()])

# # CIDErScore
# cider_score = CIDERScore()(predictions, gts)
# print(cider_score)

# CLIPScore
clip_score = CLIPScore()(predictions, images_root)
print(clip_score)
# print(f"CIDEr: {cider_score} | CLIPScore: {clip_score}")

# 0.73968505859375

