# -*- coding: utf-8 -*-
"""dlcv_hw3_p2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K-rhXGfmT2HNo3067lc6a2SdOW69k4K6

# data & setup
"""

# from google.colab import drive
# drive.mount('/content/drive')
# %cd /content/drive/MyDrive/DLCV/hw3/p2
# !unzip /content/drive/MyDrive/DLCV/hw3/hw3_data.zip

# !gdown 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR -O hw3_data.zip
# !unzip hw3_data.zip

# !unzip images.zip

# %cd /content

# !pip install loralib

# import loralib as lora

# !pip install timm

import os
import json
import torch
import torch.nn as nn
from torch.nn import MultiheadAttention
import timm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision.transforms import functional as TF
from torch.utils.data import DataLoader, Dataset
# from torch.cuda.amp import GradScaler, autocast
from tqdm.auto import tqdm

import torchvision.transforms as transforms

from tokenizer import BPETokenizer

# from typing import Tuple
# from copy import deepcopy
# from utils import PositionalEncoding

"""# dataset"""

tokenizer = BPETokenizer("./encoder.json", "vocab.bpe")
class InferenceDataset:
    def __init__(self, data_dir):
        super(InferenceDataset, self).__init__()
        self.data_dir = data_dir
        self.files = sorted([p for p in os.listdir(data_dir)])
        self.transform = transforms.Compose(
            [
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
            ]
        )

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        fname = self.files[idx]
        img = Image.open(os.path.join(self.data_dir, fname))
        img = self.transform(img)
        if img.size(0) != 3:
            img = torch.cat((img, img, img))
        return img, fname


class ImgDataset:
    def __init__(self, json_path, tfms, image_folder):
        with open(json_path) as f:
            data = json.load(f)
        self.tfms = tfms
        self.image_folder = image_folder

        self.caption_to_image_id = {annotation['caption']: annotation['image_id'] for annotation in data['annotations']}

        # Map image IDs to file names
        self.id_to_filename = {image['id']: image['file_name'] for image in data['images']}

    def __len__(self):
        return len(self.caption_to_image_id)

    def __getitem__(self, idx):
        # Get a list of all captions and select one based on the index
        captions = list(self.caption_to_image_id.keys())
        selected_caption = captions[idx]

        # Find the corresponding image ID
        image_id = self.caption_to_image_id[selected_caption]

        # Use the mapping to get the file name
        image_file = self.id_to_filename[image_id]

        # Construct the image file path
        image_path = f"{self.image_folder}/{image_file}"

        # Load and process the image
        image = Image.open(image_path).convert('RGB')
        image = self.tfms(image)

        # Process the caption
        input_ids = tokenizer.encode(selected_caption)
        input_ids = [50256] + input_ids + [50256]
        labels = input_ids.copy()
        labels[:] = input_ids[1:]
        # print(f"lables: {labels}, input_ids: {input_ids}")
        input_ids = input_ids[:80]
        input_ids += [50256] * (80 - len(input_ids))
        # print(f"input_ids: {input_ids}")
        labels = labels[:80]
        labels += [-100] * (80 - len(labels))
        # print(f"labels: {labels}")

        input_ids = torch.tensor(input_ids)
        labels = torch.tensor(labels)

        return image, input_ids, labels, image_id, image_file

"""# model"""

import math
import collections
import torch
from torch import nn, Tensor
import torch.nn.functional as F

class Config:
    def __init__(self, checkpoint=None):
        self.n_layer = 12
        self.n_head = 12
        self.n_embd = 768
        self.vocab_size = 50257
        self.block_size = 1024
        self.checkpoint = checkpoint

class CrossAttention(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.embed_dim = cfg.n_embd
        self.num_heads = cfg.n_head
        self.head_dim = self.embed_dim // self.num_heads
        assert self.embed_dim % self.num_heads == 0, "Embedding dimension must be divisible by number of heads"
        # self.q = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.k = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.v = lora.Linear(self.embed_dim, self.embed_dim, r=16)
        # self.c_proj = lora.Linear(self.embed_dim, self.embed_dim, r=16)

        self.q = nn.Linear(self.embed_dim, self.embed_dim)
        self.k = nn.Linear(self.embed_dim, self.embed_dim)
        self.v = nn.Linear(self.embed_dim, self.embed_dim)
        self.scale = self.head_dim ** -0.5
        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim)

    def forward(self, x, context):
        b, t, _ = x.size()
        q = self.q(x).view(b, t, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k(context).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v(context).view(b, -1, self.num_heads, self.head_dim).transpose(1, 2)
        attn_scores = (q @ k.transpose(-2, -1)) * self.scale
        # print(f"attn_scores.shape: {attn_scores.shape}")
        # print((self.bias[:, :, :t, :t] == 0).shape)
        # attn_scores = attn_scores.masked_fill(self.bias[:, :, :t, :t] == 0, float('-inf'))

        attn_probs = F.softmax(attn_scores, dim=-1)
        # print(f"attn_probs.shape: {attn_probs.shape}")

        return self.c_proj((attn_probs @ v).transpose(1, 2).contiguous().view(b, t, self.embed_dim)), attn_probs


class Attention(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        # self.c_attn = lora.Linear(cfg.n_embd, 3 * cfg.n_embd, r=32)
        # self.c_proj = lora.Linear(cfg.n_embd, cfg.n_embd, r=16)
        self.c_attn = nn.Linear(cfg.n_embd, 3 * cfg.n_embd)
        self.c_proj = nn.Linear(cfg.n_embd, cfg.n_embd)
        self.n_head = cfg.n_head
        self.n_embd = cfg.n_embd
        size = cfg.block_size
        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))

    def forward(self, x):
        B, T, C = x.size() # batch, context, embedding
        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))
        att = F.softmax(att, dim=-1)
        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))

class Block(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.ln_1 = nn.LayerNorm(cfg.n_embd)
        self.ln_2 = nn.LayerNorm(cfg.n_embd)
        self.ln_3 = nn.LayerNorm(cfg.n_embd)

        self.attn = Attention(cfg)
        self.cross_attn = CrossAttention(cfg)
        self.mlp = nn.Sequential(collections.OrderedDict([
            # ('c_fc', lora.Linear(cfg.n_embd, 4 * cfg.n_embd, r=32)),
            ('c_fc', nn.Linear(cfg.n_embd, 4 * cfg.n_embd)),
            ('act', nn.GELU(approximate='tanh')),
            # ('c_proj', lora.Linear(4 * cfg.n_embd, cfg.n_embd, r=32)),
            ('c_proj', nn.Linear(4 * cfg.n_embd, cfg.n_embd))
        ]))
        # only add lora in the last few mlp blocks

    def forward(self, x, context):
        x = x + self.attn(self.ln_1(x))
        # x = x + self.cross_attn(self.ln_1(x), context)
        # x = x + self.mlp(self.ln_2(x))
        x0, attn_prob = self.cross_attn(self.ln_2(x), context)
        x = x + x0
        x = x + self.mlp(self.ln_3(x))
        return x, attn_prob

class Decoder(nn.Module):

    def __init__(self, cfg):
        super().__init__()
        self.cfg = cfg
        self.block_size = cfg.block_size
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(cfg.vocab_size, cfg.n_embd),
            wpe = nn.Embedding(cfg.block_size, cfg.n_embd),
            h = nn.Sequential(*[Block(cfg) for _ in range(cfg.n_layer)]),
            ln_f = nn.LayerNorm(cfg.n_embd)
        ))
        # self.lm_head = lora.Linear(cfg.n_embd, cfg.vocab_size, r=16)
        self.lm_head = nn.Linear(cfg.n_embd, cfg.vocab_size, bias=False)
        self.transformer.wte.weight = self.lm_head.weight
        # load checkpoint
        if self.cfg.checkpoint is not None:
            state_dict = torch.load(self.cfg.checkpoint)
            transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]
            for key, value in state_dict.items():
                if any(key.endswith(w) for w in transposed):
                    state_dict[key] = value.t()
            self.transformer.load_state_dict(state_dict, strict=False)

    def forward(self, x: Tensor, encoder_output: Tensor):
        x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))
        pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)
        x = self.transformer.wte(x) + self.transformer.wpe(pos)
        # x = self.lm_head(self.transformer.ln_f(self.transformer.h(x)))
        attn_probs = 0
        for block in self.transformer.h:
            x0, attn_probs = block(x, encoder_output)  # Pass both x and encoder_output to each block
            x = x0

        x = self.lm_head(self.transformer.ln_f(x))
        # x = self.lm_head(self.transformer.ln_f(self.transformer.h(x, encoder_output)))
        return x, attn_probs

class VisionLanguageModel(nn.Module):
    def __init__(self, decoder_cfg):
        super(VisionLanguageModel, self).__init__()
        self.vit = timm.create_model('vit_large_patch14_224_clip_laion2b', pretrained=True, num_classes=0)
        # vit = timm.create_model('vit_large_patch14_clip_336', pretrained=True, num_classes=0)
        # self.patch_embed = vit.patch_embed
        # self.cls_token = nn.Parameter(vit.cls_token)
        # self.pos_embed = nn.Parameter(vit.pos_embed)
        # self.pos_drop = nn.Dropout(p=0.0)
        # self.blocks = nn.ModuleList([self.vit.blocks[i] for i in range(decoder_cfg.n_layer)])
        self.decoder = Decoder(decoder_cfg)
        # self.output_reshape = lora.Linear(1024, 768, r=16)
        self.output_reshape = nn.Linear(1024, 768)

    def forward(self, images, input_ids):
        # x = self.patch_embed(images)  # Convert images to patches
        # cls_tokens = self.cls_token.expand(images.shape[0], -1, -1)  # Expand class token to batch
        # x = torch.cat((cls_tokens, x), dim=1)  # Concat class token with patch embeddings
        # x = x + self.pos_embed  # Add positional embeddings
        # x = self.pos_drop(x)

        # for block in self.blocks:
        #     x = block(x)  # Pass through each ViT block


        # forward_feature
        x = self.vit.forward_features(images)
        x = self.output_reshape(x)
        decoder_output, attn_probs = self.decoder(input_ids, x)

        return decoder_output, attn_probs

"""# load training data"""

batch_size = 16
val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

train_json_path = './hw3_data/p2_data/train.json'
val_json_path = './hw3_data/p2_data/val.json'

train_image_folder = './hw3_data/p2_data/images/train'
train_dataset = ImgDataset(train_json_path, val_transform, train_image_folder)
print("train Dataset size:", len(train_dataset))
train_loader = DataLoader(train_dataset, batch_size, shuffle=True)

val_image_folder = './hw3_data/p2_data/images/val'
val_dataset = ImgDataset(val_json_path, val_transform, val_image_folder)
print("val Dataset size:", len(val_dataset))
val_loader = DataLoader(val_dataset, batch_size, shuffle=True)


# max_caption_length = 50

# t = tokenizer.encode("endoftext")
# print(t)
# dl = DataLoader(val_dataset,shuffle=True,batch_size=8)

# _,c,l,_ = next(iter(dl))
# print(c[0])
# print(l[0])
# t = tokenizer.decode(c[0].tolist())
# print(t)
# # print(tokenizer.decode(l[0]).tolist())

"""# train"""

import gc
from torch.cuda.amp import GradScaler
class Trainer:
    def __init__(self, decoder_cfg, train_config, dls):
        self.train_config = train_config
        self.decoder_cfg = decoder_cfg
        self.device = self.train_config.device

        # Initialize the VisionLanguageModel
        self.model = VisionLanguageModel(self.decoder_cfg).to(self.device)
        for name, param in self.model.named_parameters():
            if "output_reshape" not in name and "cross_attn" not in name:
                param.requires_grad = False

        # lora.mark_only_lora_as_trainable(self.model)
        self.train_dl, self.val_dl = dls

        # Optimizer and Scheduler
        self.optim = torch.optim.AdamW(self.model.parameters(), lr=self.train_config.lr)
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optim,
            max_lr=self.train_config.lr,
            epochs=self.train_config.epochs,
            steps_per_epoch=len(self.train_dl)
        )

    def train_one_epoch(self):
        self.model.train()
        total_loss = 0.0

        for images, input_ids, labels, _, _ in tqdm(self.train_dl, desc="Training"):
            images, input_ids, labels = images.to(self.device), input_ids.to(self.device), labels.to(self.device)

            self.optim.zero_grad(set_to_none=True)
            outputs, _ = self.model(images, input_ids)
            loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.size(-1)), labels.view(-1))
            loss.backward()
            total_loss += loss.item()
            self.optim.step()

            # self.scaler.scale(loss).backward()
            # self.scaler.step(self.optim)
            # self.scaler.update()
            # self.scheduler.step()
            # self.optim.zero_grad(set_to_none=True)

        avg_loss = total_loss / len(self.train_dl)
        return avg_loss

    @torch.no_grad()
    def validate_one_epoch(self, save_path=None, param_path = None, load_pretrain = False):
        if load_pretrain:
              self.model.load_state_dict(torch.load(param_path), strict=False)
        self.model.eval()
        predictions = {}
        # predictions = []
        total_loss = 0.0
        for images, input_ids, labels, image_ids, filenames in tqdm(self.val_dl, desc="Validating"):
            images, input_ids, labels = images.to(self.device), input_ids.to(self.device), labels.to(self.device)

            outputs, _ = self.model(images, input_ids)
            # Extract attention scores from the last cross-attention layer
            # cross_attn_layer = self.model.decoder.transformer.h[-1].cross_attn
            # attn_scores = cross_attn_layer.attn_scores

            loss = nn.CrossEntropyLoss()(outputs.view(-1, outputs.size(-1)), labels.view(-1))

            total_loss += loss.item()
            pred_tokens = outputs.argmax(dim=-1).tolist()
            labels = labels.tolist()
            image_ids = image_ids.tolist()
            for token, label, image_id, filename in zip(pred_tokens, labels, image_ids, filenames):
                if 50256 in token:
                    token = token[:token.index(50256)]
                t = tokenizer.decode(token)
                # filename = filename.replace(".jpg","")
                # predictions[filename] = t
                filename = filename.split('.')[0]
                predictions[filename] = t
                # predictions.append({'prediction': t, 'filename': filename})
            self.scheduler.step()

        avg_loss = total_loss / len(self.val_dl)
        if save_path:
            with open(save_path, 'w') as f:
                json.dump(predictions, f, indent=4)
        return avg_loss

    def fit(self, json_path = None, param_path = None, param_lora_path = None, load_pretrain = False):
        best_val_loss = float('inf')
        trainable_weights = [name for name, param in self.model.named_parameters() if param.requires_grad == True]
        if load_pretrain:
            self.model.load_state_dict(torch.load(param_path), strict=False)
            # self.model.load_state_dict(torch.load(param_lora_path), strict=False)
        for epoch in range(self.train_config.epochs):
            train_loss = self.train_one_epoch()
            val_loss = self.validate_one_epoch(f"{epoch}" + json_path)

            print(f"Epoch {epoch+1}/{self.train_config.epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}")

            # Additional logic for saving best model, early stopping, etc. can be added here
            # if val_loss < best_val_loss:
                # best_val_loss = val_loss
                # torch.save(lora.lora_state_dict(self.model), param_lora_path)
            save_weights = {k: v.cpu() for k, v in self.model.state_dict().items() if k in trainable_weights}
            torch.save(save_weights, f"./drive/MyDrive/DLCV/hw3/p2/p2_{epoch}_model.bin")

        # Final clean up
        # gc.collect()
        # torch.cuda.empty_cache()

class TrainConfig:
    def __init__(self):
        self.lr = 0.0005
        self.epochs = 3
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
config = Config(checkpoint = "./hw3_data/p2_data/decoder_model.bin")
train_config = TrainConfig()
trainer = Trainer(config, train_config, (train_loader, val_loader))

# v = trainer.validate_one_epoch("val_predict.json")
# v = trainer.validate_one_epoch("val_predict.json", "4_best_model_params.bin", True)
v = trainer.fit(json_path = "predict.json",param_path = "./drive/MyDrive/DLCV/hw3/p2/p2_0_model.bin" , load_pretrain = True )
# v = trainer.fit(json_path = "predict.json",param_path = "best_model_params.bin", load_pretrain = False )
print(v)

gc.collect()
torch.cuda.empty_cache()



"""# cider & clip score"""

# !pip install git+https://github.com/openai/CLIP.git
import clip

# !pip install git+https://github.com/bckim92/language-evaluation.git
import language_evaluation

from collections import defaultdict

def readJSON(file_path):
    try:
        with open(file_path) as f:
            data = json.load(f)
        return data
    except:
        return None

def getGTCaptions(annotations):
    img_id_to_name = {}
    for img_info in annotations["images"]:
        img_name = img_info["file_name"].replace(".jpg", "")
        img_id_to_name[img_info["id"]] = img_name

    img_name_to_gts = defaultdict(list)
    for ann_info in annotations["annotations"]:
        img_id = ann_info["image_id"]
        img_name = img_id_to_name[img_id]
        img_name_to_gts[img_name].append(ann_info["caption"])
    return img_name_to_gts


class CIDERScore:
    def __init__(self):
        self.evaluator = language_evaluation.CocoEvaluator(coco_types=["CIDEr"])

    def __call__(self, predictions, gts):
        """
        Input:
            predictions: dict of str
            gts:         dict of list of str
        Return:
            cider_score: float
        """
        # Collect predicts and answers
        predicts = []
        answers = []
        for img_name in predictions.keys():
            predicts.append(predictions[img_name])
            answers.append(gts[img_name])

        # Compute CIDEr score
        results = self.evaluator.run_evaluation(predicts, answers)
        return results['CIDEr']


class CLIPScore:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()

    def __call__(self, predictions, images_root):
        """
        Input:
            predictions: dict of str
            images_root: str
        Return:
            clip_score: float
        """
        total_score = 0.

        for img_name, pred_caption in predictions.items():
            image_path = os.path.join(images_root, f"{img_name}.jpg")
            image = Image.open(image_path).convert("RGB")

            total_score += self.getCLIPScore(image, pred_caption)
        return total_score / len(predictions)

    def getCLIPScore(self, image, caption):
        """
        This function computes CLIPScore based on the pseudocode in the slides.
        Input:
            image: PIL.Image
            caption: str
        Return:
            cilp_score: float
        """
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        text_input = clip.tokenize([caption]).to(self.device)

        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            text_features = self.model.encode_text(text_input)

        cos_sim = torch.nn.functional.cosine_similarity(image_features, text_features).item()
        return 2.5 * max(cos_sim, 0)

pred_file = "/content/3predict.json"
annotation_file = "/content/hw3_data/p2_data/val.json"
# Read data
predictions = readJSON(pred_file)
annotations = readJSON(annotation_file)
images_root = "hw3_data/p2_data/images/val"
# Preprocess annotation file
gts = getGTCaptions(annotations)

# Check predictions content is correct
assert type(predictions) is dict
assert set(predictions.keys()) == set(gts.keys())
assert all([type(pred) is str for pred in predictions.values()])

# CIDErScore
cider_score = CIDERScore()(predictions, gts)
print(cider_score)

# CLIPScore
clip_score = CLIPScore()(predictions, images_root)

print(f"CIDEr: {cider_score} | CLIPScore: {clip_score}")

#
# CIDEr:0.6587535742784347
#
# CIDEr: 0.676268771952214 | CLIPScore: 0.6959602778153822