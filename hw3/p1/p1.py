# -*- coding: utf-8 -*-
"""dlcv_hw3_p1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ohM5nC4jggkkH8VcwQ2aBfEKb0k1bzr-
"""

# Download dataset
# !gdown 11rP6KmR5Qwjhx0rfag0b5TZGBTRuPtQR -O hw3_data.zip
# Unzip the downloaded zip file
# !unzip ./hw3_data.zip

import os
import json
import torch
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision.transforms import functional as TF
from torch.utils.data import DataLoader, Dataset
from tqdm.auto import tqdm

# !pip install git+https://github.com/openai/CLIP.git
import clip

# Device configuration for model
device_type = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, clip_preprocess = clip.load('ViT-B/16', device_type)
# Output path for saving the report
report_path = "report.csv"

"""# Data"""

class ImageDataset(Dataset):
    def __init__(self, data_directory):
        self.image_paths = sorted([
            os.path.join(data_directory, filename)
            for filename in os.listdir(data_directory)
            if filename.lower().endswith('.png')
        ])
        self.image_filenames = sorted([
            filename
            for filename in os.listdir(data_directory)
            if filename.lower().endswith('.png')
        ])
    def __len__(self):
        return len(self.image_paths)
    def __getitem__(self, index):
        image = Image.open(self.image_paths[index])
        image_tensor = TF.to_tensor(image)
        label = self.image_filenames[index].split("_")[0]
        return image_tensor, label, self.image_filenames[index]

image_dataset = ImageDataset("hw3_data/p1_data/val")
image_loader = DataLoader(image_dataset, batch_size=1, shuffle=True, pin_memory=True)

# Load class labels from JSON file
with open("hw3_data/p1_data/id2label.json") as file:
    label_classes = json.load(file)

# Variables for tracking predictions
prediction_count = 0
predictions_df = pd.DataFrame({"filename": [], "label": []})

for image_tensor, label, filename in tqdm(image_loader, position=0, leave=True):
    # Prepare image input for CLIP
    pil_image = TF.to_pil_image(image_tensor.squeeze(0))
    clip_image_input = clip_preprocess(pil_image).unsqueeze(0).to(device_type)

    # Prepare text inputs for CLIP model
    text_inputs = torch.cat([clip.tokenize(f"a photo of a {class_name}") for _, class_name in label_classes.items()]).to(device_type)

    # Calculate features and similarity
    with torch.no_grad():
        image_features = clip_model.encode_image(clip_image_input)
        text_features = clip_model.encode_text(text_inputs)

    # Normalize and find top label
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    top_values, top_indices = similarity[0].topk(1)
    top_indices = top_indices.detach().cpu().numpy().astype(str)

    # Add prediction to DataFrame
    filename_str = filename[0] if isinstance(filename, list) else filename
    prediction_row = pd.DataFrame({"filename": filename, "label": top_indices[0]}, index=[0])
    # prediction_row = pd.DataFrame({"filename": [filename], "label": [top_indices[0]]})
    predictions_df = pd.concat([predictions_df, prediction_row])

# Save predictions to CSV
predictions_df.to_csv(report_path, index=False)

# Load the CSV file
predictions_df = pd.read_csv('output.csv')

# Function to extract label from filename
def extract_label_from_filename(filename):
    return filename.split('_')[0]

# Calculate accuracy
correct_predictions = 0
total_predictions = len(predictions_df)

for _, row in predictions_df.iterrows():
    filename_label = extract_label_from_filename(row['filename'])  # Extracts the label part and handles [' ']
    if str(row['label']) == filename_label:
        correct_predictions += 1

accuracy = correct_predictions / total_predictions

print(f"Accuracy: {accuracy:.2f} or {accuracy*100:.2f}%")

prediction_count = 0
# Process images in the dataset
for image_tensor, label, filename in tqdm(image_loader, position=0, leave=True):
    prediction_count += 1
    if prediction_count > 3: break  # Limit to 3 images for demonstration

    label_name = label_classes[label[0]]
    pil_image = TF.to_pil_image(image_tensor.squeeze(0))
    clip_image_input = clip_preprocess(pil_image).unsqueeze(0).to(device_type)

    # Prepare text inputs for CLIP model
    text_inputs = torch.cat([clip.tokenize(f"a photo of a {class_name}") for _, class_name in label_classes.items()]).to(device_type)

    # Calculate features and similarity
    with torch.no_grad():
        image_features = clip_model.encode_image(clip_image_input)
        text_features = clip_model.encode_text(text_inputs)

    # Normalize and calculate similarity
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    similarity_scores = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    top_values, top_indices = similarity_scores[0].topk(5)
    top_values, top_indices = top_values.detach().cpu().numpy(), top_indices.detach().cpu().numpy().astype(str)
    top_labels = [f"a photo of {label_classes[index]}" for index in top_indices]

    # Visualization of results
    fig, (image_axis, bar_axis) = plt.subplots(1, 2, figsize=(10, 5))
    image_axis.imshow(pil_image)
    bar_axis.barh(np.arange(5), 100 * top_values)
    bar_axis.set_yticks(np.arange(5), labels=top_labels, ha='left')
    bar_axis.invert_yaxis()
    bar_axis.tick_params(axis="y", direction="in", pad=-100)
    plt.savefig(f"visualization_{prediction_count}.png")
    plt.clf()